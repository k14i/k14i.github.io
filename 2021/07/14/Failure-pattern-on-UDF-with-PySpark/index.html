<!DOCTYPE html><html lang="ja"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="alternative" href="/atom.xml" title="kt.log" type="application/atom+xml"><link rel="icon" href="/favicon.png"><title>Failure pattern on UDF with PySpark - kt.log</title>
<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="/js/fancybox/jquery.fancybox.min.css">
<!--[if lt IE 9]><script>(function(a,b){a="abbr article aside audio bdi canvas data datalist details dialog figcaption figure footer header hgroup main mark meter nav output progress section summary template time video".split(" ");for(b=a.length-1;b>=0;b--)document.createElement(a[b])})()</script><![endif]-->
<script src="/js/jquery-3.1.1.min.js"></script>

<script src="/js/fancybox/jquery.fancybox.min.js"></script>
<meta name="generator" content="Hexo 6.2.0"></head><body style="opacity:0"><header class="head"><h1 class="head-title u-fl"><a href="/">kt.log</a></h1><nav class="head-nav u-fr"><ul class="head-nav__list"><li class="head-nav__item"><a class="head-nav__link" href="/archives">Articles</a></li><li class="head-nav__item"><a class="head-nav__link" href="/tags">Tags</a></li><li class="head-nav__item"><a class="head-nav__link" href="/links">Links</a></li></ul></nav></header><main class="main"><article class="post"><header class="post__head"><time class="post__time" datetime="2021-07-13T16:11:16.000Z">2021-07-14 01:11:16</time><h1 class="post__title"><a href="/2021/07/14/Failure-pattern-on-UDF-with-PySpark/">Failure pattern on UDF with PySpark</a></h1><div class="post__main echo"><a href="/2021/07/13/Insert-data-into-Azure-Managed-Instance-for-Apache-Cassandra-on-Azure-Databricks-in-Python/" title="Insert data into Azure Managed Instance for Apache Cassandra on Azure Databricks in Python">Insert data into Azure Managed Instance for Apache Cassandra on Azure Databricks in Python</a> や <a href="/2021/07/13/Transform-and-load-data-into-Azure-Managed-Instance-for-Apache-Cassandra-on-Azure-Databricks-in-Python/" title="Transform and load data into Azure Managed Instance for Apache Cassandra on Azure Databricks in Python">Transform and load data into Azure Managed Instance for Apache Cassandra on Azure Databricks in Python</a> の まとめ にて、並行処理に関するチャレンジに関して一言記述しました。
<p>上記記事では成功するアプローチで執筆していますが、本記事では、うまくいかなかったアプローチについて紹介したいと思います。</p>
<h1 id="前提"><a href="#前提" class="headerlink" title="前提"></a>前提</h1><ul>
<li><a href="/2021/07/13/Transform-and-load-data-into-Azure-Managed-Instance-for-Apache-Cassandra-on-Azure-Databricks-in-Python/" title="Transform and load data into Azure Managed Instance for Apache Cassandra on Azure Databricks in Python">Transform and load data into Azure Managed Instance for Apache Cassandra on Azure Databricks in Python</a> における UDF 関連のコードの意図を理解している</li>
</ul>
<h1 id="免責"><a href="#免責" class="headerlink" title="免責"></a>免責</h1><ul>
<li>筆者の環境やアプローチではうまくいかなかったのですが、何らかの工夫をすることで、本記事のアプローチを成功に導ける可能性はあり得ます。</li>
</ul>
<h1 id="解説"><a href="#解説" class="headerlink" title="解説"></a>解説</h1><a href="/2021/07/13/Insert-data-into-Azure-Managed-Instance-for-Apache-Cassandra-on-Azure-Databricks-in-Python/" title="Insert data into Azure Managed Instance for Apache Cassandra on Azure Databricks in Python">Insert data into Azure Managed Instance for Apache Cassandra on Azure Databricks in Python</a> や <a href="/2021/07/13/Transform-and-load-data-into-Azure-Managed-Instance-for-Apache-Cassandra-on-Azure-Databricks-in-Python/" title="Transform and load data into Azure Managed Instance for Apache Cassandra on Azure Databricks in Python">Transform and load data into Azure Managed Instance for Apache Cassandra on Azure Databricks in Python</a> では、 Cassandra テーブルに対する CQL 実行を、 Azure Databricks の Notebook 上でのイテレーションで実現していました。以下のような感じです。

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">dataCollect = df.rdd.toLocalIterator()</span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> dataCollect:</span><br><span class="line">  <span class="keyword">try</span>:</span><br><span class="line">    cassandra_session.execute(prepared_session,</span><br><span class="line">                              (row[<span class="number">0</span>], row[<span class="number">1</span>], row[<span class="number">2</span>], row[<span class="number">3</span>], row[<span class="number">4</span>], row[<span class="number">5</span>], row[<span class="number">6</span>], row[<span class="number">7</span>], row[<span class="number">8</span>], row[<span class="number">9</span>], row[<span class="number">10</span>], row[<span class="number">11</span>], row[<span class="number">12</span>]))</span><br><span class="line">  <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(e)</span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>

<p>このアプローチは成功しますが、パフォーマンスの観点でいうと全くメリットがありません。</p>
<p>当初は Spark のメリットを活かすために、以下のようなコードを書いていました。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> UserDefinedType</span><br><span class="line"><span class="keyword">import</span> pyspark.sql.functions <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">_function = <span class="keyword">lambda</span> row: cassandra_session.execute(prepared_session,</span><br><span class="line">                                                  (row[<span class="number">0</span>], row[<span class="number">1</span>], row[<span class="number">2</span>], row[<span class="number">3</span>], row[<span class="number">4</span>], row[<span class="number">5</span>], row[<span class="number">6</span>], row[<span class="number">7</span>], row[<span class="number">8</span>], row[<span class="number">9</span>], row[<span class="number">10</span>], row[<span class="number">11</span>], row[<span class="number">12</span>]))</span><br><span class="line">_returnType = UserDefinedType</span><br><span class="line">_udf = F.udf(_function, _returnType())</span><br><span class="line">df.select(_udf())</span><br></pre></td></tr></table></figure>

<p>このコードの実行結果はエラーで、その内容は以下の通りです。</p>
<img src="/2021/07/14/Failure-pattern-on-UDF-with-PySpark/Failure-pattern-on-UDF-with-PySpark_00001.png" class="slug">

<p>ちなみに、上記 UDF を使うアプローチ以外にも、 <code>df.foreach()</code> や <code>df.rdd.map()</code> で <code>cassandra_session.execute(...)</code> しようとした際にも、同じエラーが発生しています。<br>一方、 <code>cassandra_session.execute(...)</code> とは異なる、ありふれたコードをこれらで並行処理させようとすると、このようなエラーは出ずに Spark の処理が成功します。</p>
<p>このことから、 Spark に並行処理させるコードの中に、 Cassandra のセッション情報を含めることができないということが分かりました。</p>
<p>これは推測ですが、 <code>cassandra_session</code> はシングルトンになっていて、それを手元の Notebook の Python インタプリタから Spark に渡す際の PySpark によるシリアライズに失敗しているという状況なのだと思われます。<br>筆者は過去、別のインタプリタ言語で似たようなリモート処理を実装しようとして、やはりシリアライズに失敗した経験があります。もしかしたら、 Python ではなく Scala で記述することで、ブレークスルーできるかもしれません。</p>
<h1 id="まとめ"><a href="#まとめ" class="headerlink" title="まとめ"></a>まとめ</h1><p>Cassandra へのクエリを Spark で並行処理しようとして、うまくいかなかったパターンについてご紹介しました。<br>もしこれを改善できるアイディアがあれば、共有いただけると幸いです。</p>
<h1 id="See-also"><a href="#See-also" class="headerlink" title="See also"></a>See also</h1><ul>
<li><a target="_blank" rel="noopener" href="http://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.functions.udf.html">pyspark.sql.functions.udf — PySpark 3.1.1 documentation</a></li>
</ul>
</div></header><footer class="post__foot u-cf"><ul class="post__tag u-fl"><li class="post__tag__item"><a class="post__tag__link" href="/tags/PySpark/">PySpark</a></li></ul></footer></article></main><footer class="foot"><div class="foot-copy">&copy; 2021 - 2023 kt.log<a class="icp-a" href="https://github.com/k14i" target="view_window">by kt (Keisuke Takahashi)</a></div></footer><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));
ga('create','G-9XBBR7M7V2');
ga('send','pageview');</script>
<script src="/js/scroller.js"></script>

<script src="/js/main.js"></script>
</body></html>
<!DOCTYPE html><html lang="ja"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="alternative" href="/atom.xml" title="kt.log" type="application/atom+xml"><link rel="icon" href="/favicon.png"><title>Failure pattern on parallelization with PySpark - kt.log</title>
<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="/js/fancybox/jquery.fancybox.min.css">
<!--[if lt IE 9]><script>(function(a,b){a="abbr article aside audio bdi canvas data datalist details dialog figcaption figure footer header hgroup main mark meter nav output progress section summary template time video".split(" ");for(b=a.length-1;b>=0;b--)document.createElement(a[b])})()</script><![endif]-->
<script src="/js/jquery-3.1.1.min.js"></script>

<script src="/js/fancybox/jquery.fancybox.min.js"></script>
<meta name="generator" content="Hexo 5.4.0"></head><body style="opacity:0"><header class="head"><h1 class="head-title u-fl"><a href="/">kt.log</a></h1><nav class="head-nav u-fr"><ul class="head-nav__list"><li class="head-nav__item"><a class="head-nav__link" href="/archives">Articles</a></li><li class="head-nav__item"><a class="head-nav__link" href="/tags">Tags</a></li><li class="head-nav__item"><a class="head-nav__link" href="/links">Links</a></li></ul></nav></header><main class="main"><article class="post"><header class="post__head"><time class="post__time" datetime="2021-07-13T15:23:55.000Z">2021-07-14 00:23:55</time><h1 class="post__title"><a href="/2021/07/14/Failure-pattern-on-parallelization-with-PySpark/">Failure pattern on parallelization with PySpark</a></h1><div class="post__main echo"><a href="/2021/07/13/Insert-data-into-Azure-Managed-Instance-for-Apache-Cassandra-on-Azure-Databricks-in-Python/" title="Insert data into Azure Managed Instance for Apache Cassandra on Azure Databricks in Python">Insert data into Azure Managed Instance for Apache Cassandra on Azure Databricks in Python</a> や <a href="/2021/07/13/Transform-and-load-data-into-Azure-Managed-Instance-for-Apache-Cassandra-on-Azure-Databricks-in-Python/" title="Transform and load data into Azure Managed Instance for Apache Cassandra on Azure Databricks in Python">Transform and load data into Azure Managed Instance for Apache Cassandra on Azure Databricks in Python</a> の まとめ にて、並行処理に関するチャレンジに関して一言記述しました。
<p>上記記事では成功するアプローチで執筆していますが、本記事では、うまくいかなかったアプローチについて紹介したいと思います。</p>
<h1 id="前提"><a href="#前提" class="headerlink" title="前提"></a>前提</h1><ul>
<li><a href="/2021/07/13/Transform-and-load-data-into-Azure-Managed-Instance-for-Apache-Cassandra-on-Azure-Databricks-in-Python/" title="Transform and load data into Azure Managed Instance for Apache Cassandra on Azure Databricks in Python">Transform and load data into Azure Managed Instance for Apache Cassandra on Azure Databricks in Python</a> における UDF 関連のコードの意図を理解している</li>
</ul>
<h1 id="免責"><a href="#免責" class="headerlink" title="免責"></a>免責</h1><ul>
<li>筆者の環境やアプローチではうまくいかなかったのですが、何らかの工夫をすることで、本記事のアプローチを成功に導ける可能性はあり得ます。</li>
</ul>
<h1 id="解説"><a href="#解説" class="headerlink" title="解説"></a>解説</h1><a href="/2021/07/13/Transform-and-load-data-into-Azure-Managed-Instance-for-Apache-Cassandra-on-Azure-Databricks-in-Python/" title="Transform and load data into Azure Managed Instance for Apache Cassandra on Azure Databricks in Python">Transform and load data into Azure Managed Instance for Apache Cassandra on Azure Databricks in Python</a> では、Spark を利用したハッシュ計算の並行処理について、 UDF を利用する形で、以下のアプローチをとりました。

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StringType</span><br><span class="line"><span class="keyword">import</span> pyspark.sql.functions <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> hashlib</span><br><span class="line"></span><br><span class="line">df = spark.createDataFrame(rows.<span class="built_in">all</span>())</span><br><span class="line"></span><br><span class="line">_udf = <span class="keyword">lambda</span> lat, lon, yr, mo, d, hr: hashlib.sha256(</span><br><span class="line">         <span class="string">&quot;&quot;</span>.join([</span><br><span class="line">             <span class="built_in">str</span>(lat),</span><br><span class="line">             <span class="built_in">str</span>(lon),</span><br><span class="line">             <span class="built_in">str</span>(yr),</span><br><span class="line">             <span class="built_in">str</span>(mo),</span><br><span class="line">             <span class="built_in">str</span>(d),</span><br><span class="line">             <span class="built_in">str</span>(hr),</span><br><span class="line">         ]).encode(<span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line">       ).hexdigest()</span><br><span class="line">udf = F.udf(_udf, StringType())</span><br><span class="line"></span><br><span class="line">new_df = df.withColumn(<span class="string">&quot;sha256hash&quot;</span>, udf(<span class="string">&quot;latitude&quot;</span>, <span class="string">&quot;longitude&quot;</span>, <span class="string">&quot;year&quot;</span>, <span class="string">&quot;month&quot;</span>, <span class="string">&quot;day&quot;</span>, <span class="string">&quot;forecasthour&quot;</span>))</span><br></pre></td></tr></table></figure>

<p>筆者は当初 UDF を使わず、関数型言語的な考えで、以下のような処理を試みました。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> Row</span><br><span class="line">df = spark.sparkContext.parallelize(rows.<span class="built_in">all</span>()).<span class="built_in">map</span>(<span class="keyword">lambda</span> x: Row(x)).toDF()</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> hashlib</span><br><span class="line">hashes = df.rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: \</span><br><span class="line">           hashlib.sha256(</span><br><span class="line">             <span class="string">&quot;&quot;</span>.join([</span><br><span class="line">                  <span class="built_in">str</span>(x[<span class="number">0</span>][<span class="string">&#x27;latitude&#x27;</span>]),</span><br><span class="line">                  <span class="built_in">str</span>(x[<span class="number">0</span>][<span class="string">&#x27;longitude&#x27;</span>]),</span><br><span class="line">                  <span class="built_in">str</span>(x[<span class="number">0</span>][<span class="string">&#x27;year&#x27;</span>]),</span><br><span class="line">                  <span class="built_in">str</span>(x[<span class="number">0</span>][<span class="string">&#x27;month&#x27;</span>]),</span><br><span class="line">                  <span class="built_in">str</span>(x[<span class="number">0</span>][<span class="string">&#x27;day&#x27;</span>]),</span><br><span class="line">                  <span class="built_in">str</span>(x[<span class="number">0</span>][<span class="string">&#x27;forecasthour&#x27;</span>]),</span><br><span class="line">                ]).encode(<span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line">           ).hexdigest()</span><br><span class="line">         ).collect()</span><br><span class="line"></span><br><span class="line">new_df = df.withColumn(<span class="string">&quot;sha256hash&quot;</span>, hashes)</span><br></pre></td></tr></table></figure>

<p>結果としてこちらはうまくいきませんでした。 <code>x[0][&#39;year&#39;]</code>, <code>x[0][&#39;month&#39;]</code>, <code>x[0][&#39;day&#39;]</code>, <code>x[0][&#39;forecasthour&#39;]</code> の値が <code>None</code> になってしまい、結果として <code>hashes</code> のリスト内にあるハッシュ値は全て同じ値となってしまいました。（なお、当該記事では <code>latitude</code> と <code>longitude</code> はある値を決め打ちで指定しています。）<br>色々と試行錯誤して検証しましたが、 <code>Row(x)</code> が float と double 以外の型を扱えないのが原因であると結論づけました。</p>
<h1 id="まとめ"><a href="#まとめ" class="headerlink" title="まとめ"></a>まとめ</h1><p>Spark の UDF を使わずに <code>spark.sparkContext.parallelize</code> を使って関数型言語的アプローチをとった結果、うまくいかなかったパターンについてご紹介しました。<br>もしこれを改善できるアイディアがあれば、共有いただけると幸いです。</p>
<h1 id="See-also"><a href="#See-also" class="headerlink" title="See also"></a>See also</h1><ul>
<li><a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.Row.html">pyspark.sql.Row — PySpark 3.1.2 documentation</a></li>
</ul>
</div></header><footer class="post__foot u-cf"><ul class="post__tag u-fl"><li class="post__tag__item"><a class="post__tag__link" href="/tags/PySpark/">PySpark</a></li></ul></footer></article></main><footer class="foot"><div class="foot-copy">&copy; 2021 - 2022 kt.log<a class="icp-a" href="https://github.com/k14i" target="view_window">by kt (Keisuke Takahashi)</a></div></footer><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));
ga('create','G-9XBBR7M7V2');
ga('send','pageview');</script>
<script src="/js/scroller.js"></script>

<script src="/js/main.js"></script>
</body></html>
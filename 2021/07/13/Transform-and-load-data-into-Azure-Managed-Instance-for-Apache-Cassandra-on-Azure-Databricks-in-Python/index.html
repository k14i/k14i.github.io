<!DOCTYPE html><html lang="ja"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="alternative" href="/atom.xml" title="kt.log" type="application/atom+xml"><link rel="icon" href="/favicon.png"><title>Transform and load data into Azure Managed Instance for Apache Cassandra on Azure Databricks in Python - kt.log</title>
<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="/js/fancybox/jquery.fancybox.min.css">
<!--[if lt IE 9]><script>(function(a,b){a="abbr article aside audio bdi canvas data datalist details dialog figcaption figure footer header hgroup main mark meter nav output progress section summary template time video".split(" ");for(b=a.length-1;b>=0;b--)document.createElement(a[b])})()</script><![endif]-->
<script src="/js/jquery-3.1.1.min.js"></script>

<script src="/js/fancybox/jquery.fancybox.min.js"></script>
<meta name="generator" content="Hexo 6.2.0"></head><body style="opacity:0"><header class="head"><h1 class="head-title u-fl"><a href="/">kt.log</a></h1><nav class="head-nav u-fr"><ul class="head-nav__list"><li class="head-nav__item"><a class="head-nav__link" href="/archives">Articles</a></li><li class="head-nav__item"><a class="head-nav__link" href="/tags">Tags</a></li><li class="head-nav__item"><a class="head-nav__link" href="/links">Links</a></li></ul></nav></header><main class="main"><article class="post"><header class="post__head"><time class="post__time" datetime="2021-07-13T06:38:01.000Z">2021-07-13 15:38:01</time><h1 class="post__title"><a href="/2021/07/13/Transform-and-load-data-into-Azure-Managed-Instance-for-Apache-Cassandra-on-Azure-Databricks-in-Python/">Transform and load data into Azure Managed Instance for Apache Cassandra on Azure Databricks in Python</a></h1><div class="post__main echo"><a href="/2021/07/13/Insert-data-into-Azure-Managed-Instance-for-Apache-Cassandra-on-Azure-Databricks-in-Python/" title="Insert data into Azure Managed Instance for Apache Cassandra on Azure Databricks in Python">Insert data into Azure Managed Instance for Apache Cassandra on Azure Databricks in Python</a> では、 Azure Managed Instance for Apache Cassandra クラスターのテーブルに対して、 Azure Databricks クラスターから Python でデータを INSERT する方法を解説しました。
<p>本記事では、 Azure Managed Instance for Apache Cassandra から Extract したデータを Azure Databricks で Transform し、 Cassandra に Load する方法について解説します。 Transform したデータを Cassandra の別テーブルに INSERT することをゴールとします。</p>
<h1 id="前提"><a href="#前提" class="headerlink" title="前提"></a>前提</h1><ul>
<li><a target="_blank" rel="noopener" href="https://azure.microsoft.com/ja-jp/">Microsoft Azure</a> に利用可能なサブスクリプションを持っている</li>
<li>その他、 <a href="/2021/07/13/Insert-data-into-Azure-Managed-Instance-for-Apache-Cassandra-on-Azure-Databricks-in-Python/" title="Insert data into Azure Managed Instance for Apache Cassandra on Azure Databricks in Python">Insert data into Azure Managed Instance for Apache Cassandra on Azure Databricks in Python</a> における前提と同様</li>
</ul>
<h1 id="免責"><a href="#免責" class="headerlink" title="免責"></a>免責</h1><ul>
<li>本記事では処理の厳密さを追求していないため、INSERT の際にエラーが発生した場合は無視をして INSERT を継続することとしています。</li>
</ul>
<h1 id="手順"><a href="#手順" class="headerlink" title="手順"></a>手順</h1><p>本記事で行う Transform 処理についてですが、今回は単純に、既存レコードからハッシュ値を計算して、各レコードに付加する処理とします。 <a href="/2021/07/13/Insert-data-into-Azure-Managed-Instance-for-Apache-Cassandra-on-Azure-Databricks-in-Python/" title="先の記事">先の記事</a> で扱ったデータにはユニークなキーが無いため、ハッシュ値をもってキーとするのがコンセプトです。</p>
<h2 id="Extract"><a href="#Extract" class="headerlink" title="Extract"></a>Extract</h2><p>Azure Managed Instance for Apache Cassandra に INSERT 済みの NOAA Global Forecast System データを SELECT してきます。ここでは特定のレコード5件に絞って実行します。（当該データの場合、ある latitude と longitude の組み合わせにかかる1日分のデータは93件になります。）</p>
<h3 id="CQL-の作成"><a href="#CQL-の作成" class="headerlink" title="CQL の作成"></a>CQL の作成</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">keyspace_name = <span class="string">&quot;_&quot;</span>.join([os.environ[<span class="string">&#x27;CASSANDRA_KEYSPACE_NAME&#x27;</span>],</span><br><span class="line">                          os.environ[<span class="string">&#x27;CASSANDRA_KEYSPACE_NAME_PREFIX&#x27;</span>]])</span><br><span class="line">table_name = os.environ[<span class="string">&#x27;CASSANDRA_TABLE_NAME_FOR_SOURCE_DATA&#x27;</span>]</span><br><span class="line"></span><br><span class="line">target_latitude = -<span class="number">13</span></span><br><span class="line">target_longitude = <span class="number">269.5</span></span><br><span class="line">select_limit = <span class="number">5</span></span><br><span class="line"></span><br><span class="line">query_select_data = <span class="string">&quot;SELECT * FROM &#123;&#125;.&#123;&#125; WHERE latitude = &#123;&#125; AND longitude = &#123;&#125; LIMIT &#123;&#125;;&quot;</span>\</span><br><span class="line">                    .<span class="built_in">format</span>(keyspace_name, table_name, target_latitude, target_longitude, select_limit)</span><br></pre></td></tr></table></figure>

<h3 id="SELECT-の実行"><a href="#SELECT-の実行" class="headerlink" title="SELECT の実行"></a>SELECT の実行</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> cassandra.query <span class="keyword">import</span> ordered_dict_factory</span><br><span class="line"></span><br><span class="line">cassandra_session.row_factory = ordered_dict_factory</span><br><span class="line">rows = cassandra_session.execute(query_select_data)</span><br></pre></td></tr></table></figure>


<h2 id="Transform"><a href="#Transform" class="headerlink" title="Transform"></a>Transform</h2><h3 id="Spark-DataFrame-への変換"><a href="#Spark-DataFrame-への変換" class="headerlink" title="Spark DataFrame への変換"></a>Spark DataFrame への変換</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df = spark.createDataFrame(rows.<span class="built_in">all</span>())</span><br></pre></td></tr></table></figure>

<p>DataFrame の中身は以下の通りです。</p>
<img src="/2021/07/13/Transform-and-load-data-into-Azure-Managed-Instance-for-Apache-Cassandra-on-Azure-Databricks-in-Python/Transform-and-load-data-into-Azure-Managed-Instance-for-Apache-Cassandra-on-Azure-Databricks-in-Python_00001.png" class="slug">

<h3 id="UDF-を使用した-Transform"><a href="#UDF-を使用した-Transform" class="headerlink" title="UDF を使用した Transform"></a>UDF を使用した Transform</h3><p>Spark では UDF (User Defined Functions) が使用できます。今回はデータ加工処理を Spark 側で UDF として定義し、それを使って処理を実行することで、 Spark の並行処理を活かしたいと思います。</p>
<h4 id="UDF-の定義"><a href="#UDF-の定義" class="headerlink" title="UDF の定義"></a>UDF の定義</h4><p>当該データは <code>latitude</code>, <code>longitude</code>, <code>year</code>, <code>month</code>, <code>day</code>, <code>forecasthour</code> の組み合わせでユニークになりますので、これらを元に sha256 ハッシュを生成します。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StringType</span><br><span class="line"><span class="keyword">import</span> pyspark.sql.functions <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> hashlib</span><br><span class="line"></span><br><span class="line">_udf = <span class="keyword">lambda</span> lat, lon, yr, mo, d, hr: hashlib.sha256(</span><br><span class="line">         <span class="string">&quot;&quot;</span>.join([</span><br><span class="line">             <span class="built_in">str</span>(lat),</span><br><span class="line">             <span class="built_in">str</span>(lon),</span><br><span class="line">             <span class="built_in">str</span>(yr),</span><br><span class="line">             <span class="built_in">str</span>(mo),</span><br><span class="line">             <span class="built_in">str</span>(d),</span><br><span class="line">             <span class="built_in">str</span>(hr),</span><br><span class="line">         ]).encode(<span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line">       ).hexdigest()</span><br><span class="line">udf = F.udf(_udf, StringType())</span><br></pre></td></tr></table></figure>

<h4 id="定義した-UDF-を使った処理の実行"><a href="#定義した-UDF-を使った処理の実行" class="headerlink" title="定義した UDF を使った処理の実行"></a>定義した UDF を使った処理の実行</h4><p>UDF を使って処理した結果は、元の <code>df</code> に新しいカラム <code>sha256hash</code> として連結します。そしてそれを <code>new_df</code> という新しい DataFrame として扱います。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">new_df = df.withColumn(<span class="string">&quot;sha256hash&quot;</span>, udf(<span class="string">&quot;latitude&quot;</span>, <span class="string">&quot;longitude&quot;</span>, <span class="string">&quot;year&quot;</span>, <span class="string">&quot;month&quot;</span>, <span class="string">&quot;day&quot;</span>, <span class="string">&quot;forecasthour&quot;</span>))</span><br><span class="line">new_df = new_df.select(<span class="string">&quot;sha256hash&quot;</span>, <span class="string">&quot;latitude&quot;</span>, <span class="string">&quot;longitude&quot;</span>, <span class="string">&quot;year&quot;</span>, <span class="string">&quot;month&quot;</span>, <span class="string">&quot;day&quot;</span>, <span class="string">&quot;forecasthour&quot;</span>, <span class="string">&quot;currentdatetime&quot;</span>,</span><br><span class="line">                       <span class="string">&quot;precipitablewaterentireatmosphere&quot;</span>, <span class="string">&quot;sealvlpressure&quot;</span>, <span class="string">&quot;temperature&quot;</span>, <span class="string">&quot;totalcloudcoverconvectivecloud&quot;</span>, <span class="string">&quot;windspeedgustsurface&quot;</span>)</span><br></pre></td></tr></table></figure>

<p><code>new_df</code> のスキーマは以下の通りになります。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">sha256hash:string</span><br><span class="line">latitude:double</span><br><span class="line">longitude:double</span><br><span class="line">year:long</span><br><span class="line">month:long</span><br><span class="line">day:long</span><br><span class="line">forecasthour:long</span><br><span class="line">currentdatetime:timestamp</span><br><span class="line">precipitablewaterentireatmosphere:double</span><br><span class="line">sealvlpressure:double</span><br><span class="line">temperature:double</span><br><span class="line">totalcloudcoverconvectivecloud:double</span><br><span class="line">windspeedgustsurface:double</span><br></pre></td></tr></table></figure>

<p>また、 <code>new_df</code> のデータは以下のようになりました。</p>
<img src="/2021/07/13/Transform-and-load-data-into-Azure-Managed-Instance-for-Apache-Cassandra-on-Azure-Databricks-in-Python/Transform-and-load-data-into-Azure-Managed-Instance-for-Apache-Cassandra-on-Azure-Databricks-in-Python_00002.png" class="slug">


<h2 id="Load"><a href="#Load" class="headerlink" title="Load"></a>Load</h2><p><code>new_df</code> を Azure Managed Instance for Apache Cassandra クラスターの新しいテーブルに INSERT します。</p>
<h3 id="テーブルの作成"><a href="#テーブルの作成" class="headerlink" title="テーブルの作成"></a>テーブルの作成</h3><p>この段階ではまだ新しいテーブルが存在しないため、<a href="/2021/07/12/Create-Azure-Managed-Instance-for-Apache-Cassandra-schema-on-Azure-Databricks-in-Python/" title="先の記事">先の記事</a> と同じ要領で <code>new_df</code> 用の新しいテーブルを作成します。</p>
<p>新しいテーブル名は予め環境変数で以下のように定義しています。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CASSANDRA_TABLE_NAME_FOR_DESTINATION_DATA=noaa_gfs_weather_with_sha256hash</span><br></pre></td></tr></table></figure>

<p>Keyspace 名 <code>keyspace_name</code> およびその他環境変数は、先の記事と同じものを使い回します。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">table_name = os.environ[<span class="string">&#x27;CASSANDRA_TABLE_NAME_FOR_DESTINATION_DATA&#x27;</span>]</span><br><span class="line">c = new_df.columns</span><br><span class="line"></span><br><span class="line">query_create_table = <span class="string">&quot;CREATE TABLE IF NOT EXISTS &#123;&#125;.&#123;&#125; (\</span></span><br><span class="line"><span class="string">    &#123;&#125; ascii,\</span></span><br><span class="line"><span class="string">    &#123;&#125; float,\</span></span><br><span class="line"><span class="string">    &#123;&#125; float,\</span></span><br><span class="line"><span class="string">    &#123;&#125; smallint,\</span></span><br><span class="line"><span class="string">    &#123;&#125; tinyint,\</span></span><br><span class="line"><span class="string">    &#123;&#125; tinyint,\</span></span><br><span class="line"><span class="string">    &#123;&#125; smallint,\</span></span><br><span class="line"><span class="string">    &#123;&#125; timestamp,\</span></span><br><span class="line"><span class="string">    &#123;&#125; double,\</span></span><br><span class="line"><span class="string">    &#123;&#125; double,\</span></span><br><span class="line"><span class="string">    &#123;&#125; double,\</span></span><br><span class="line"><span class="string">    &#123;&#125; double,\</span></span><br><span class="line"><span class="string">    &#123;&#125; double,\</span></span><br><span class="line"><span class="string">    PRIMARY KEY((&#123;&#125;), &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;)\</span></span><br><span class="line"><span class="string">    ) WITH CLUSTERING ORDER BY (&#123;&#125; DESC, &#123;&#125; DESC, &#123;&#125; DESC, &#123;&#125; DESC, &#123;&#125; DESC, &#123;&#125; DESC)\</span></span><br><span class="line"><span class="string">    AND comment = &#x27;&#123;&#125;&#x27;\</span></span><br><span class="line"><span class="string">    AND gc_grace_seconds = &#123;&#125;;\</span></span><br><span class="line"><span class="string">    &quot;</span>.<span class="built_in">format</span>(keyspace_name, table_name,</span><br><span class="line">             c[<span class="number">0</span>], c[<span class="number">1</span>], c[<span class="number">2</span>], c[<span class="number">3</span>], c[<span class="number">4</span>], c[<span class="number">5</span>], c[<span class="number">6</span>], c[<span class="number">7</span>], c[<span class="number">8</span>], c[<span class="number">9</span>], c[<span class="number">10</span>], c[<span class="number">11</span>], c[<span class="number">12</span>],</span><br><span class="line">             c[<span class="number">0</span>], c[<span class="number">1</span>], c[<span class="number">2</span>], c[<span class="number">3</span>], c[<span class="number">4</span>], c[<span class="number">5</span>], c[<span class="number">6</span>],</span><br><span class="line">             c[<span class="number">1</span>], c[<span class="number">2</span>], c[<span class="number">3</span>], c[<span class="number">4</span>], c[<span class="number">5</span>], c[<span class="number">6</span>],</span><br><span class="line">             <span class="string">&quot;This table is for NOAA Global Forecast System (GFS) data from Azure Open Datasets.&quot;</span>,</span><br><span class="line">             os.environ[<span class="string">&#x27;CASSANDRA_TABLE_GC_GRACE_SECONDS&#x27;</span>])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cassandra_session.execute(query_create_table)</span><br></pre></td></tr></table></figure>

<h3 id="データの-INSERT"><a href="#データの-INSERT" class="headerlink" title="データの INSERT"></a>データの INSERT</h3><p>こちらも先の記事と同じ要領で、新しいテーブルに対して INSERT をします。</p>
<h4 id="プリペアドステートメントの作成"><a href="#プリペアドステートメントの作成" class="headerlink" title="プリペアドステートメントの作成"></a>プリペアドステートメントの作成</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">prepared_statement = <span class="string">&quot;INSERT INTO &#123;&#125;.&#123;&#125; \</span></span><br><span class="line"><span class="string">    (&#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;)\</span></span><br><span class="line"><span class="string">    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\</span></span><br><span class="line"><span class="string">    &quot;</span>.<span class="built_in">format</span>(keyspace_name, table_name,</span><br><span class="line">             c[<span class="number">0</span>], c[<span class="number">1</span>], c[<span class="number">2</span>], c[<span class="number">3</span>], c[<span class="number">4</span>], c[<span class="number">5</span>], c[<span class="number">6</span>], c[<span class="number">7</span>], c[<span class="number">8</span>], c[<span class="number">9</span>], c[<span class="number">10</span>], c[<span class="number">11</span>], c[<span class="number">12</span>])</span><br><span class="line">prepared_session = cassandra_session.prepare(prepared_statement)</span><br></pre></td></tr></table></figure>

<h4 id="INSERT-の実行"><a href="#INSERT-の実行" class="headerlink" title="INSERT の実行"></a>INSERT の実行</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">dataCollect = new_df.rdd.toLocalIterator()</span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> dataCollect:</span><br><span class="line">  <span class="keyword">try</span>:</span><br><span class="line">    cassandra_session.execute(prepared_session, (row[<span class="number">0</span>], row[<span class="number">1</span>], row[<span class="number">2</span>], row[<span class="number">3</span>], row[<span class="number">4</span>], row[<span class="number">5</span>], row[<span class="number">6</span>], row[<span class="number">7</span>], row[<span class="number">8</span>], row[<span class="number">9</span>], row[<span class="number">10</span>], row[<span class="number">11</span>], row[<span class="number">12</span>]))</span><br><span class="line">  <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(e)</span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>

<h4 id="結果の確認"><a href="#結果の確認" class="headerlink" title="結果の確認"></a>結果の確認</h4><img src="/2021/07/13/Transform-and-load-data-into-Azure-Managed-Instance-for-Apache-Cassandra-on-Azure-Databricks-in-Python/Transform-and-load-data-into-Azure-Managed-Instance-for-Apache-Cassandra-on-Azure-Databricks-in-Python_00003.png" class="slug">


<h1 id="まとめ"><a href="#まとめ" class="headerlink" title="まとめ"></a>まとめ</h1><p>Azure Managed Instance for Apache Cassandra から Extract したデータを Azure Databricks で Transform し、 Cassandra に Load する方法について解説しました。特に Transform にあたっては、 Spark の UDF は強力な存在と言えると思います。<br>一方で、 Load の部分で Spark のメリットが活かせていないのですが、これは先の記事でも触れた通りです。この話は別の記事で触れたいと思います。</p>
<h1 id="See-also"><a href="#See-also" class="headerlink" title="See also"></a>See also</h1><ul>
<li><a target="_blank" rel="noopener" href="https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.SparkSession.createDataFrame">SparkSession.createDataFrame(data, schema=None, samplingRatio=None, verifySchema=True)</a></li>
<li><a target="_blank" rel="noopener" href="http://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.functions.udf.html">pyspark.sql.functions.udf — PySpark 3.1.1 documentation</a></li>
</ul>
</div></header><footer class="post__foot u-cf"><ul class="post__tag u-fl"><li class="post__tag__item"><a class="post__tag__link" href="/tags/Azure/">Azure</a></li><li class="post__tag__item"><a class="post__tag__link" href="/tags/Azure-Managed-Instance-for-Apache-Cassandra/">Azure Managed Instance for Apache Cassandra</a></li><li class="post__tag__item"><a class="post__tag__link" href="/tags/Azure-Databricks/">Azure Databricks</a></li><li class="post__tag__item"><a class="post__tag__link" href="/tags/Python/">Python</a></li></ul></footer></article></main><footer class="foot"><div class="foot-copy">&copy; 2021 - 2022 kt.log<a class="icp-a" href="https://github.com/k14i" target="view_window">by kt (Keisuke Takahashi)</a></div></footer><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));
ga('create','G-9XBBR7M7V2');
ga('send','pageview');</script>
<script src="/js/scroller.js"></script>

<script src="/js/main.js"></script>
</body></html>
<!DOCTYPE html><html lang="ja"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="alternative" href="/atom.xml" title="kt.log" type="application/atom+xml"><link rel="icon" href="/favicon.png"><title>Insert data into Azure Managed Instance for Apache Cassandra on Azure Databricks in Python - kt.log</title>
<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="/js/fancybox/jquery.fancybox.min.css">
<!--[if lt IE 9]><script>(function(a,b){a="abbr article aside audio bdi canvas data datalist details dialog figcaption figure footer header hgroup main mark meter nav output progress section summary template time video".split(" ");for(b=a.length-1;b>=0;b--)document.createElement(a[b])})()</script><![endif]-->
<script src="/js/jquery-3.1.1.min.js"></script>

<script src="/js/fancybox/jquery.fancybox.min.js"></script>
<meta name="generator" content="Hexo 5.4.0"></head><body style="opacity:0"><header class="head"><h1 class="head-title u-fl"><a href="/">kt.log</a></h1><nav class="head-nav u-fr"><ul class="head-nav__list"><li class="head-nav__item"><a class="head-nav__link" href="/archives">Articles</a></li><li class="head-nav__item"><a class="head-nav__link" href="/tags">Tags</a></li><li class="head-nav__item"><a class="head-nav__link" href="/links">Links</a></li></ul></nav></header><main class="main"><article class="post"><header class="post__head"><time class="post__time" datetime="2021-07-13T05:23:46.000Z">2021-07-13 14:23:46</time><h1 class="post__title"><a href="/2021/07/13/Insert-data-into-Azure-Managed-Instance-for-Apache-Cassandra-on-Azure-Databricks-in-Python/">Insert data into Azure Managed Instance for Apache Cassandra on Azure Databricks in Python</a></h1><div class="post__main echo"><a href="/2021/07/12/Create-Azure-Managed-Instance-for-Apache-Cassandra-schema-on-Azure-Databricks-in-Python/" title="前の記事">前の記事</a> では Azure Managed Instance for Apache Cassandra クラスターでのスキーマ作成を、 Azure Databricks クラスターから Python で実行する方法について解説しました。
<p>本記事では、同じ環境において、作成したテーブルに対してデータを INSERT する方法について解説します。 INSERT がエラー無く成功することをゴールとします。なお、データには引き続き Azure Open Datasets の NOAA Global Forecast System を使用します。</p>
<h1 id="前提"><a href="#前提" class="headerlink" title="前提"></a>前提</h1><ul>
<li><a target="_blank" rel="noopener" href="https://azure.microsoft.com/ja-jp/">Microsoft Azure</a> に利用可能なサブスクリプションを持っている</li>
<li><a href="/2021/07/12/Create-Azure-Managed-Instance-for-Apache-Cassandra-schema-on-Azure-Databricks-in-Python/" title="Create Azure Managed Instance for Apache Cassandra schema on Azure Databricks in Python">Create Azure Managed Instance for Apache Cassandra schema on Azure Databricks in Python</a> の手順に沿って Azure Managed Instance for Apache Cassandra クラスター上にテーブルが作成されている</li>
<li>Azure Managed Instance for Apache Cassandra との間で確立したセッションが <code>cassandra_session</code> 変数に格納されている</li>
<li>Azure Open Datasets から取得した NOAA Global Forecast System のオープンデータが Spark DataFrame 形式で <code>gfs_df</code> 変数に格納されている</li>
<li><code>gfs_df</code> のカラムのリストが <code>c</code> 変数に格納されている</li>
</ul>
<h1 id="免責"><a href="#免責" class="headerlink" title="免責"></a>免責</h1><ul>
<li>本記事では処理の厳密さを追求していないため、INSERT の際にエラーが発生した場合は無視をして INSERT を継続することとしています。</li>
</ul>
<h1 id="手順"><a href="#手順" class="headerlink" title="手順"></a>手順</h1><h2 id="フォーマット変換"><a href="#フォーマット変換" class="headerlink" title="フォーマット変換"></a>フォーマット変換</h2><p><code>gfs_df</code> の <code>currentDatetime</code> 列について、 Spark DataFrame も Cassandra テーブルも型は <code>timestamp</code> です。しかし、前者と後者とでは date と time との間に <code>T</code> が入るかどうかに違いがあり、前者を後者にそのまま INSERT しようとするとエラーになります。そのためのフォーマット変換を以下のように行います。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> TimestampType</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> unix_timestamp</span><br><span class="line">df = gfs_df.withColumn(<span class="string">&quot;currentDatetime&quot;</span>, unix_timestamp(<span class="string">&quot;currentDatetime&quot;</span>, <span class="string">&quot;yyyy-MM-dd&#x27;T&#x27;HH:mm:ss.SSSXXX&quot;</span>).cast(TimestampType()))</span><br></pre></td></tr></table></figure>

<p>変換後の DataFrame は新たに <code>df</code> という変数に代入しました。</p>
<p>型自体は <code>timestamp</code> のまま変わっていないこと、 <code>T</code> の文字が除去されていることを、以下のように確認しましょう。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.printSchema()</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line"> |-- currentDatetime: timestamp (nullable = true)</span><br><span class="line"> |-- forecastHour: integer (nullable = true)</span><br><span class="line"> |-- latitude: double (nullable = true)</span><br><span class="line"> |-- longitude: double (nullable = true)</span><br><span class="line"> |-- precipitableWaterEntireAtmosphere: double (nullable = true)</span><br><span class="line"> |-- seaLvlPressure: double (nullable = true)</span><br><span class="line"> |-- snowDepthSurface: double (nullable = true)</span><br><span class="line"> |-- temperature: double (nullable = true)</span><br><span class="line"> |-- windSpeedGustSurface: double (nullable = true)</span><br><span class="line"> |-- totalCloudCoverConvectiveCloud: double (nullable = true)</span><br><span class="line"> |-- year: integer (nullable = true)</span><br><span class="line"> |-- month: integer (nullable = true)</span><br><span class="line"> |-- day: integer (nullable = true)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.show(<span class="number">5</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+-------------------+------------+--------+---------+---------------------------------+--------------+------------------+------------------+--------------------+------------------------------+----+-----+---+</span><br><span class="line">|    currentDatetime|forecastHour|latitude|longitude|precipitableWaterEntireAtmosphere|seaLvlPressure|  snowDepthSurface|       temperature|windSpeedGustSurface|totalCloudCoverConvectiveCloud|year|month|day|</span><br><span class="line">+-------------------+------------+--------+---------+---------------------------------+--------------+------------------+------------------+--------------------+------------------------------+----+-----+---+</span><br><span class="line">|2018-12-20 00:00:00|           0|   -90.0|     79.0|                3.548314332962036| 71160.9765625|1.0099999904632568| 260.6067810058594|  12.820813179016113|                          null|2018|   12| 20|</span><br><span class="line">|2018-12-20 00:00:00|           0|   -90.0|    268.0|                3.548314332962036| 71160.9765625|1.0099999904632568| 260.6067810058594|  12.820813179016113|                          null|2018|   12| 20|</span><br><span class="line">|2018-12-20 00:00:00|           0|   -89.5|     36.5|               3.4483141899108887| 70757.7734375|1.0099999904632568| 258.6067810058594|  12.620813369750977|                          null|2018|   12| 20|</span><br><span class="line">|2018-12-20 00:00:00|           0|   -89.5|     43.0|               3.3483142852783203| 70597.7734375|1.0099999904632568| 258.3067932128906|  12.720812797546387|                          null|2018|   12| 20|</span><br><span class="line">|2018-12-20 00:00:00|           0|   -89.5|    144.0|                3.248314380645752| 69701.7734375|1.0099999904632568|259.50677490234375|  12.620813369750977|                          null|2018|   12| 20|</span><br><span class="line">+-------------------+------------+--------+---------+---------------------------------+--------------+------------------+------------------+--------------------+------------------------------+----+-----+---+</span><br><span class="line">only showing top 5 rows</span><br></pre></td></tr></table></figure>

<h2 id="プリペアドステートメントの作成と登録"><a href="#プリペアドステートメントの作成と登録" class="headerlink" title="プリペアドステートメントの作成と登録"></a>プリペアドステートメントの作成と登録</h2><p>Cassandra でもプリペアドステートメントが利用可能です。以下のように記述して実行します。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">prepared_statement = <span class="string">&quot;INSERT INTO &#123;&#125;.&#123;&#125; \</span></span><br><span class="line"><span class="string">    (&#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;)\</span></span><br><span class="line"><span class="string">    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\</span></span><br><span class="line"><span class="string">    &quot;</span>.<span class="built_in">format</span>(keyspace_name, table_name,</span><br><span class="line">             c[<span class="number">0</span>], c[<span class="number">1</span>], c[<span class="number">2</span>], c[<span class="number">3</span>], c[<span class="number">4</span>], c[<span class="number">5</span>], c[<span class="number">6</span>], c[<span class="number">7</span>], c[<span class="number">8</span>], c[<span class="number">9</span>], c[<span class="number">10</span>], c[<span class="number">11</span>], c[<span class="number">12</span>])</span><br><span class="line">prepared_session = cassandra_session.prepare(prepared_statement)</span><br></pre></td></tr></table></figure>

<p>一般的な話ですが、プリペアドステートメント自体は文字列で定義し、データベースに登録を行い、 <code>?</code> の箇所に後から値を挿入します。</p>
<h2 id="INSERT-の実行"><a href="#INSERT-の実行" class="headerlink" title="INSERT の実行"></a>INSERT の実行</h2><p>Spark DataFrame <code>df</code> を1行ずつ、プリペアドステートメントを使って Cassandra のテーブルに INSERT します。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">dataCollect = df.rdd.toLocalIterator()</span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> dataCollect:</span><br><span class="line">  <span class="keyword">try</span>:</span><br><span class="line">    cassandra_session.execute(prepared_session,</span><br><span class="line">                              (row[<span class="number">0</span>], row[<span class="number">1</span>], row[<span class="number">2</span>], row[<span class="number">3</span>], row[<span class="number">4</span>], row[<span class="number">5</span>], row[<span class="number">6</span>], row[<span class="number">7</span>], row[<span class="number">8</span>], row[<span class="number">9</span>], row[<span class="number">10</span>], row[<span class="number">11</span>], row[<span class="number">12</span>]))</span><br><span class="line">  <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(e)</span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>

<p><code>df.rdd.toLocalIterator()</code> は <code>df.collect()</code> と記述しても同様にイテレーション処理をすることができます。</p>
<h2 id="INSERT-結果の確認"><a href="#INSERT-結果の確認" class="headerlink" title="INSERT 結果の確認"></a>INSERT 結果の確認</h2><p>INSERT の完了には時間がかかります。今回、 <code>gfs_df</code> は NOAA Global Forecast System のデータ わずか1日分でしたが、概算で約1GB程の大きさがあります。 INSERT 完了まで、筆者の環境では11時間20分の時間を要しました。</p>
<p>結果はテーブルの行数で確認します。変数 <code>keyspace_name</code> <code>table_name</code> は、それぞれ Azure Managed Instance for Apache Cassandra に作成した Keyspace と テーブルの名前です。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(session.execute(<span class="string">&quot;SELECT COUNT(*) FROM &#123;&#125;.&#123;&#125;&quot;</span>.<span class="built_in">format</span>(keyspace_name, table_name)).one())</span><br></pre></td></tr></table></figure>

<p>筆者の環境では、上記のコードの実行は <code>OperationTimedOut</code> となってしまいましたが、実際にレコードを SELECT することで、データが INSERT されていることを確認しています。</p>
<h1 id="まとめ"><a href="#まとめ" class="headerlink" title="まとめ"></a>まとめ</h1><p>Azure Managed Instance for Apache Cassandra クラスター上に作成したテーブルに対して、 Azure Databricks クラスターから Python でデータを INSERT する方法について解説しました。<br>本当は <code>df</code> をイテレーションするのではなく、データセットごと Cassandra にインポートするか、 Spark にイテレーション実行の並行処理をさせたいところですが、特に後者に関してはできないようですので、別の記事で解説をしたいと思います。</p>
<h1 id="See-also"><a href="#See-also" class="headerlink" title="See also"></a>See also</h1><ul>
<li><a target="_blank" rel="noopener" href="https://towardsdatascience.com/the-most-complete-guide-to-pyspark-dataframes-2702c343b2e8">The Most Complete Guide to pySpark DataFrames</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.datastax.com/ja/dse/5.1/cql/cql/cql_reference/refDataTypes.html#refDataTypes__timestamp">CQLのデータ型 | CQL for DataStax Enterprise 5.1 &gt; timestamp</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.datastax.com/en/developer/java-driver/4.6/manual/core/statements/prepared/">DataStax Java Driver - Prepared statements</a></li>
<li><a target="_blank" rel="noopener" href="https://sparkbyexamples.com/pyspark/pyspark-loop-iterate-through-rows-in-dataframe/">PySpark - Loop/Iterate Through Rows in DataFrame — SparkByExamples</a></li>
</ul>
</div></header><footer class="post__foot u-cf"><ul class="post__tag u-fl"><li class="post__tag__item"><a class="post__tag__link" href="/tags/Azure/">Azure</a></li><li class="post__tag__item"><a class="post__tag__link" href="/tags/Azure-Managed-Instance-for-Apache-Cassandra/">Azure Managed Instance for Apache Cassandra</a></li><li class="post__tag__item"><a class="post__tag__link" href="/tags/Azure-Databricks/">Azure Databricks</a></li><li class="post__tag__item"><a class="post__tag__link" href="/tags/Python/">Python</a></li></ul></footer></article></main><footer class="foot"><div class="foot-copy">&copy; 2021 - 2022 kt.log<a class="icp-a" href="https://github.com/k14i" target="view_window">by kt (Keisuke Takahashi)</a></div></footer><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));
ga('create','G-9XBBR7M7V2');
ga('send','pageview');</script>
<script src="/js/scroller.js"></script>

<script src="/js/main.js"></script>
</body></html>
*******************************
GlusterFSの特徴を裏付ける仕組み
*******************************

====
GFID
====

GlusterFS上のファイル及びディレクトリに対して、volume内で一意に割り当てられるIDである。GlusterFS 3.1で導入された。分散配置やレプリケーションといった従来的あるいは本質的な機能では必要とされていなかったが、Geo-replicationやglustershdなど、外部からデータの管理を行う機能を追加していくために導入された。現在ではレプリケーションも含め、全体としてGFIDの存在を前提とした設計となっている。
GFIDは、後述するExtended Attributeとして付与される。名前空間は trusted.gfid である。GFIDはUUID形式であるため、128bitすなわち2^128 のパターンを扱うことができる。また、ファイルのレプリケーションが行われるvolumeにおいて、異なるbrickに存在する複製されたデータに付与されるGFIDは、同一のものである。
余談であるが、実装された GlusterFS 3.1.0以降、3.2.1までの間、レプリケーションされたデータ間でGFIDが同一にならないことがある不具合に悩まされていた経緯がある。GlusterFS史上最も長い闘いの末、その後のバージョンではこの問題は解消している。


==================
Extended Attribute
==================

先述の通り、GlusterFSが中央サーバを持たないアーキテクチャとなっているのは、Extended Attributeによるデータの管理を行っているためである。
Extended Attributeは、GlusterFSのバックエンドに存在するカーネル空間のファイルシステムが提供する機能である。すなわち、Extended Attributeに対応していることが、GlusterFSのバックエンドとして利用できるファイルシステムの要件でもある。
Extended Attributeは、iノードに記録される。そのため、ファイルサイズ自体には影響を及ぼさない。
また、シェルから操作するには、専用のコマンドを用いる。閲覧する場合は getfattr、セットしたり変更したりする場合は setfattrである。
Extended Attributeには名前空間があり、GlusterFSでは trusted.glusterfs 及び trusted.gfid が用いられる。


===
XFS
===

カーネル空間で実装された64ビットのジャーナリングファイルシステムであり、brickのバックエンドとして使用することが推奨されている。16TBを超える容量を1つのパーティションで扱うことができ、iノード数が無制限となっている。Ext4と比較してもパフォーマンスに優れているとされる。
過去にカーネルスタックに関する深刻な問題があったが、最近のバージョンではそれを回避できるアーキテクチャとなっている。
GlusterFSでは、最終的なデータの管理をこういったバックエンドのファイルシステム、それもカーネルにマージされたものに委ねることで、信頼性を高めることに成功していると言える。


=========================
Elastic Hashing Algorithm
=========================

先述の通り、ファイルの分散配置はアルゴリズムベースであり、GlusterFSにおいてはElastic Hashing Algorithmと名付けられたアルゴリズムが用いられている。

* volumeとbrickとの対応関係を表現したハッシュテーブルを保持しておく
* ファイルのパス名からハッシュ値を計算する
* 算出されたハッシュ値が、ハッシュテーブル上でどのbrickに対応するかを導き出す
* 該当するbrickの当該ファイルパスに対してデータを読み書きする

正常時における分散配置の挙動はこの通りであり、通常のハッシュアルゴリズムと同等である。
Elastic Hashing Algorithmは、その名の通り、brickの追加削除に伴うクラスタサイズの変更にも対応ができるよう、ハッシュアルゴリズムを拡張したものである。

brick追加時の挙動は、以下の通りである。

* brickの追加イベントが発生する
* 追加後と追加前とで、双方のハッシュテーブルを保持しておく
* 既存データへのアクセスには、追加前のハッシュテーブルを用いる
* 新規データの生成時には、追加後のハッシュテーブルを用いてデータを生成した上で、追加前のハッシュテーブルを用いて導き出したbrick上に、このデータの実体があるbrickを示すリンクファイルを生成する
* 生成した新規データへは、リンクファイルを参照した上で、指定されたbrickにある実体へとアクセスする
* rebalanceを実行する
* すべてのファイルについてハッシュ値の再計算が行われる。移動対象データについては、移動中のアクセスが可能となるようリンクファイルがデータの移動先に生成され、データの実体へのポインタの役割を果たす
* 移動が完了したデータから、リンクファイルを削除する
* すべてのデータの移動が完了したら、rebalanceを終了する

このように、新旧2つのハッシュテーブルを使って、volumeの拡大縮小＝brickの追加削除の各フェーズでそれらを使い分けながら、新しいハッシュテーブルに移行していく。これが、Elastic Hashing Algorithmである。

このアルゴリズムの長所としては、ダウンタイム無しで伸縮可能なハッシュアルゴリズムとしては最もシンプルであることにある。
反対に、短所としては、ボリュームサイズの変更が発生する度に、rebalanceによるファイルの移動が発生することである。rebalance中でもデータの読み書きは可能であるが、ネットワークトラフィックの増加やディスクアクセスの増加によるサービスへの影響や、rebalance完了までの間の運用監視など、考慮すべきことは少なくない。

..
  詳細な図解と特殊なケースでの挙動に関しては、以下を参照。
  http://www.nttpc.co.jp/company/r_and_d/technology/gluster_fs.html


=========
self-heal
=========

GlusterFSにおいて、brickのダウン中などにデータの追加・変更・削除が行われ、それによってレプリケーションされたデータ間で差異が発生した場合に、古い状態を検出し、新しい状態へ修正する機能を、self-healと呼ぶ。
self-healに関する主たる機能は、cluster/afrトランスレータに実装されている。これはAutomatic File Replication(AFR)を実現するモジュールであり、つまりレプリケーション機能の一部であると言える。

self-healの挙動は、以下の通りである。

* volumeのマウントポイント配下のファイル又はディレクトリに対して、open()やreaddir()等の特定のシステムコールが発行される
* レプリカを構成するbrick間で、当該データに付与されている Extended Attribute のうち trusted.glusterfs.version の差異を確認する
* 差異がある場合、バージョンが低いデータを、バージョンが高いデータに合わせる
* 差分が解消されたら、各brickの対象データに付与されている trusted.glusterfs.version の値をクリアする

差異を修正する際に、必要に応じて、ファイルの転送を行う。AFRをコントロールするのはGlusterFSクライアントであるため、AFRを実行するノードがファイル転送の橋渡しをすることとなる。
また、self-healは非同期で実行される。すなわち、ファイルの読み込みがあった場合に、ユーザに対しては、差分の解消を待たずに、バージョンが高い方のファイルが返る。ファイルの書き込みと異なり、読み込みは単一のbrickに対して処理が実行されるため、パフォーマンスを考慮してこのような挙動となっている。

以上はAFRの機能として提供されるものであるが、GlusterFS 3.3以降では、ユーザからのトリガ無しに自動的にself-healが実行される機能が追加されている。glustershdというデーモンがデフォルトで10分ごとに差分をチェックし、self-healを実行するものである。なお、self-heal時にはファイルのロックが行われるため、glustershdやAFR同士が競合することはない。


..
  参考:
  http://hekafs.org/index.php/2012/03/glusterfs-algorithms-replication-present/
  http://joejulian.name/blog/glusterfs-replication-dos-and-donts/


==============
ファイルロック
==============

先で触れたファイルのロックに関して解説する。ファイルの入出力にあたり、GlusterFSクライアントのマウントポイント配下に対してflock()又はfcntl()システムコールによるロックが発行されると、当該ファイルはGlusterFSサーバ側のfeatures/locksトランスレータ内で、POSIXミューテックスによりロックされる。GlusterFSは共有ファイルシステムであり、特定のファイルを複数のユーザが読み書きする可能性がある。また、GlusterFS内部でもperformance/io-threadsトランスレータによるスレッド処理が行われるが、ファイルロック機構により、ファイル操作のアトミック性が担保される。
なお、ユーザがNFSでGlusterFSを利用する場合にも、nfslockサービスが利用可能なLinuxディストリビューションであれば、複数のNFSクライアント間でロックを共有することができる。[#]_

.. [#] NFS version 3は、プロトコル上、ファイルのロックを要件としていない。そのため、一般的な実装においては、NFS経由でファイルをロックするシステムコールを発行しても、それは無視される。しかしながら、共有ストレージの信頼性の面からファイルロックに対するニーズはあり、nfslockサービスがこれを補完する。


*************************
GlusterFSのアーキテクチャ
*************************

GlusterFSは、オープンソースの分散ファイルシステムである。一般的にストレージはブロックストレージ、ファイルストレージ、オブジェクトストレージに大別されるが、GlusterFSはファイルストレージに分類され、その中でもPOSIX準拠のインタフェースを持つ、ファイルシステムである。
なお、このような分散データストアは、グリッドコンピューティングにおけるストレージグリッドに由来するもので、複数のノードを接続することで、単一のボリュームを実現する。

.. image:: 表1「ストレージアーキテクチャの典型的な分類」

========================
システム・アーキテクチャ
========================

--------------------------------------
他の分散ファイルシステムと比較した特徴
--------------------------------------

GlusterFSが実現するシステムのアーキテクチャについて、他のプロダクトと比較した上での特徴は、3点挙げられる。

* シンプルなサーバとリッチなクライアント
* 中央サーバを持たない
* 多様なインタフェース


シンプルなサーバとリッチなクライアント
======================================

GlusterFSは、サーバ・クライアント型のアーキテクチャを取る。サーバは、ストレージ領域を提供することに徹し、そのストレージ領域を組み合わせてクラスタを構成する機能は、クライアント側で持っている。すなわち、実際にクライアントにてファイルの入出力が行われた場合に、それをどのノードに分散配置するかの決定、又はファイルのレプリケーション（複製）の処理などを、クライアントが行う。
また、クライアントは、NFSサーバ機能の提供も行う。この場合、NFSクライアントにはGlusterFSのインストールは不要である。


中央サーバを持たない
====================

メタデータサーバやネームノードのような、中央集権的にクラスタを管理するノードを、GlusterFSは持たない。メタデータを持ってはいるが、それらはファイルやディレクトリに対してExtended Attributeとして付与される。また、後述するが、データの配置はアルゴリズムにより決定される。
このアーキテクチャにより、ノードの追加によるスループットのリニアなスケールアウトが容易に実現可能となっている。また、SPOFとなり得るノードの冗長性及び堅牢性を高める等の工夫も不要となっている。
反面、基本的にデータの分散配置を最適化すること、言い換えればデータの偏りを是正するなど、きめ細かなデータの分散ができないアーキテクチャである。

.. image:: 図1「プロセスから見るGlusterFS」

多様なインタフェース
====================

ユーザ空間実装のファイルシステムで多く使われるFUSEを利用した、GlusterネイティブプロトコルでのPOSIX準拠インタフェースに加え、NFS version 3、VFSプラグインとSambaを利用したCIFSプロトコル、Swiftのフロントエンドを組み込んだRESTfulなHTTPオブジェクトストレージ、libgfapiの提供によるC言語インタフェース、QEMUとのインテグレーションにより実現したqemu-kvmからのブロックストレージとしての利用、bd xlatorとLVMの組合せによるブロックストレージとしての利用が可能となっている。

.. image:: 図2「OSから見るGlusterFS」


----------------------------------------
分散ファイルシステムに共通する部分の特徴
----------------------------------------

ここからは、他のプロダクトと共通するアーキテクチャの解説を行う。

GlusterFSにおいて、クラスタのことをストレージプールと呼ぶ。1つのpeerは、複数のストレージプールに所属することができない。peerが1つも存在しない状況下で、あるノードから別のノードをコマンドでpeerとして指定した場合に、その2つのpeerによるストレージプールが生成される。
ストレージプールの中には、複数のvolumeを作成することができる。volumeは、brickと呼ばれるGlusterFSサーバのディレクトリパスを集約したものとなる。volumeの作成はglusterコマンドから実行する。これには通常、root権限が必要となっている。なお、brickの作成はvolumeの作成時に自動的に行われるため、特に意識する必要は無い。
ここまでの解説の範囲で、よく使用される構成でのコマンドを以下に示す。

* peerの指定

::

  # gluster peer probe <ホスト名又はIPアドレス>

* volumeの作成

::

  # gluster volume create <volume名> replica 2 <peer1>:/PATH/TO/BRICK <peer2>:/PATH/TO/BRICK ...

* volumeの起動

::

  # gluster volume start <volume名>

* volumeのマウント

::

  # mount -t glusterfs localhost:<volume名> /PATH/TO/MOUNT

この手順により、マウントポイント /PATH/TO/MOUNT にGlusterネイティブプロトコルでvolumeがマウントされる。先述の通り、GlusterFSクライアントがファイルの主要なハンドリングを行う。そのため、2レプリケーション構成である本構成においては、マウントポイントを持っている当該クライアントが、ファイルの分散配置先の決定やレプリケーションを行う。

以下、ファイルの分散配置及びレプリケーションに関連するアーキテクチャについて解説する。


ファイルの分散配置
==================

複数のノードに跨るファイルの分散配置は、分散データストアが共通して持つ基本的な機能である。分散配置が可能となることで、複数のノードが持つストレージ領域を、集約された単一のボリュームとして提供することができる。

.. image:: 図3「分散」

この分散配置をどのように実現するかが、各種分散データストアを特徴付ける共通要素であると言える。これについて大別すると、ストレージリソースの状況を把握している中央サーバがデータの配置先のノードをクライアントに指定することで、利用効率の最大化及びクライアントにかかる負荷の最小化を図るタイプと、中央サーバを持たずにアルゴリズムベースでデータを分散することで、オーバーヘッドの最小化、性能のリニアなスケールアウト及びSPOFの排除を図るタイプがある。GlusterFSは後者に分類される。

GlusterFSは、DHT(分散ハッシュテーブル)を利用して、ファイルを分散配置する。DHTのアルゴリズムは、コンシステントハッシュ法をベースにGlusterFSプロジェクトで独自に開発したElastic Hashing Algorithmである。詳細は後述する。

..
  【参考，掲載対象外】
  * http://hekafs.org/index.php/2012/03/glusterfs-algorithms-distribution/
  * https://confluence.oceanobservatories.org/download/attachments/30998760/An_Introduction_To_Gluster_ArchitectureV7_110708.pdf
  * https://events.linuxfoundation.org/images/stories/pdf/lcjp2012_black.pdf


データのレプリケーション
========================

一方、データのレプリケーションは、分散データストアとしては必須ではないが、多くのそれらが共通して持つ機能である。レプリケーションのポイントは2点、データをどのように転送するかと、データをどのように保持するかにある。


データをどのように転送するか
----------------------------

まず、データのレプリケーションにおいて、それらをどのように転送するかについて解説する。
GlusterFSにおいて、データの複製を行うのは、Glusterネイティブのクライアントである。これは、ユーザがNFSでGlusterFSをマウントする場合も同様である。[#]_

.. [#] この場合、GlusterネイティブのクライアントがNFSサーバ機能を提供する。そのため、NFSクライアントはレプリケーションを意識することは無い。

Glusterネイティブクライアントは、入力されたデータ（ファイル）をコピーすることで、volumeに予め定義された数にまで複製を行い、それぞれ異なるbrickへと転送する。どのbrickに転送するかは、volumeの構成に従って静的に決定される。[#]_

.. [#] gluster volume create コマンドの引数に渡したbrickの順序と、replicaを指定した場合はその数値によって、データの複製が行われるbrickの組が決定される。ノードを追加する場合は、指定したreplicaの数に応じて、brickの組を指定する必要があり、そのbrick同士でデータの複製が行われることとなる。

この転送処理は、データの複製を保持するbrickの組に対して、並行して行われる。後述するが、GlusterFSにおけるデータ保持の方式上、ファイルの書き込みにあたっては、レプリカの数に応じたトラフィックが発生することに留意する必要がある。すなわち、クライアント・サーバ間でのトラフィックはレプリカ数に比例、言い換えれば、各レプリカのスループットはレプリカ数に反比例することとなる。例えば、レプリカ数が2である場合、GlusterFSクライアントからは2つのbrickに対して同時に同じデータを2つ書き込むこととなる。この場合、スループットが1Gbpsのネットワークを使用していても、GlusterFSクライアントにとっては、実行スループットは512Mbps相当となる。この点、バックエンドでレプリケーションが行われる分散データストアと比較すると、短所になると言えるが、その反面、同期的な書き込みによるレプリケーションの確実性 [#]_ が、エンタープライズ分野で選ばれる理由の一つとなっているのもまた事実である。

.. [#] どのソフトウェアも確実性は設計上担保しているが、非同期処理の難しさに起因する実装ミスにより、レプリケーションが行われないケースは存在する。

なお、転送中に書き込み先の一部のbrick上で障害が発生した場合、そのbrickは各クライアントから切断され、残りのbrickに対してデータの転送が継続されることとなる。GlusterFSのクラスタ内において、ノードは互いにヘルスチェックを行っており、プロセスダウン程度であれば即座に、ネットワーク障害であればタイムアウト（デフォルトで42秒）を待ってから、障害ノードの切り離しが行われる。なお、切り離されたノードの代替となるノードをクラスタ内から補填する機能は無く、残されたノードで機能提供を継続することとなる。


データをどのように保持するか
----------------------------

複製されるデータをどのように保持するかに関して、一般的な方式は2つに大別される。RAID 1のように同じブロック，ファイル，又はチャンクをミラーリングして保持する方式、そしてRAID 5やRAID 6のようにデータを加工し、パリティ等何らかのリカバリ手段を用意した上でデータを保持する方式である。なお、後者の方式の一つとしてErasure codingが最近注目を集めている。GlusterFSは、前者の方式のうち、複数のノードで同一のファイルをミラーリングする方式を採用している。

いずれの方式にも長所と短所があるが、GlusterFSが享受している長所としては、データの保全性であろう。GlusterFSのバックエンドにはKernel空間で実装された一般的なファイルシステム（Ext4やXFSなど）があり、データはファイル単位でそこに格納される。そのため、万が一GlusterFSが壊れて動作しなくなってしまった場合においても、各brick上からGlusterFSを介さずにデータへアクセスすることが可能である。[#]_

.. [#] GlusterFSとしては想定していない使い方であり、通常利用では推奨されない。

この点、分散データストアがエンタープライズ分野において歴史が浅い中、GlusterFSが選ばれる大きな理由の一つである。実際、過去にGlusterFSクライアントが高負荷・無応答となった場合でも、データが消失していないことを、各brick上からローカルファイルシステムを参照する方法で確認した事例は複数存在する。このような運用が可能であるのも、GlusterFSがバックエンドにファイルシステムを利用し、データの保持をそこに一任しているためであると言える。

一方で、このレプリケーション方式の短所としては、レプリカ数に応じてデータ量が膨れ上がることにある。標準である2レプリケーション構成において、容量に対する効率は良くはなく、GlusterFS 3.6で機能追加されるdisperseトランスレータ（Erasure codeの実装）が期待される。


self-heal
---------

また、GlusterFSはレプリケーション機能を利用してデータの修復機能である「self-heal」を実現している。これにより、一部ノードが障害から復旧した際に、データが自動的に正しい状態へと修復される。

.. image:: 図4「レプリケーションとself-heal」

詳細は後述する。


============================
ソフトウェア・アーキテクチャ
============================

ここまでシステムとしてのアーキテクチャについて述べたが、一歩踏み込んで、ソフトウェア内部のアーキテクチャについても触れたい。


------------
拡張性の高さ
------------

GlusterFSは、C言語で記述されており、しかしながら、コードはオブジェクト指向で書かれている。GlusterFSの大きな特徴の一つは、クリーンで再利用が容易なコードがもたらす拡張性にあると言える。GlusterFSの機能は「トランスレータ」としてモジュール化されており、この組み合わせによって、まさに"Software Defined Storage"を実現するものである。モジュールの組み合わせはvolumeごとに定義でき、その定義をgraphという。GlusterFSは、バージョン3.0まではこのgraphを設定ファイルとして記述しなければならなかったが、それ以後のバージョンでは、管理者がgraphを意識する必要はなくなっている。

以下に、graphの一例を示す。なお、本稿で扱わないモジュールについては記載を省略した。

.. image:: 図5「GlusterFSのgraph構造(概要)」

入力されたデータや命令を、複数のトランスレータがそれぞれの処理を行うことで、GlusterFSはストレージとしての機能を提供する。このトランスレータを記述し、差し込んだり入れ替えたりすることで、GlusterFSはその機能を自在に変更できるアーキテクチャとなっている。
そのため、GlusterFSの機能追加スピードは、他の競合ソフトウェア等と比較しても非常に早く、機能の内容も自由度が高くなっている。

なお、GlusterFS 3.1以降では、ユーザ側でトランスレータを独自開発する際の敷居が上がってしまったが、GlusterFS 3.7以降で、そのような独自トランスレータを導入するための改善が予定されている。また、既存のGluPy（グルーピー）という仕組みを利用することで、Pythonで簡単に独自トランスレータを記述することもできる。このように、ソフトウェア・アーキテクチャが実現する高い柔軟性も、GlusterFSの魅力の一つである。


--------------------
ユーザランドでの実装
--------------------

ソフトウェア・アーキテクチャに関して、GlusterFSの開発の自由度やスピードを支える要因はもう一つある。それは、GlusterFSがユーザランドで実装されているということである。これにより、カーネルランドで実装するよりも遥かに手軽に実装することができる。他のツールとの連携もしやすい。
しかし、ユーザランドでの実装にはデメリットもある。カーネルランドで実装するよりも、性能面で不利であるということである。
GlusterFSで入出力されるデータは、TCP/IPネットワークを経由するだけでなく、クライアントとサーバの両方で、ユーザ空間とカーネル空間との間のメモリ内コピーが発生する。そもそも分散ファイルシステムとして大容量のデータの入出力が主たる用途となっているGlusterFSにおいて、このアーキテクチャは性能面でネガティブな影響を与える。
多くの導入ケースでGlusterFSの性能評価が行われており、意外と良いという評価も、意外と悪いという評価もある。これは性能要件やベンチマーク方法によって変わってくるものだが、思ったよりも性能が出ない場合には、このソフトウェア・アーキテクチャを加味して考察を行うと良いだろう。

